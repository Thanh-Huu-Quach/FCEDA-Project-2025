import json
import os
import io

import boto3
import pandas as pd

s3 = boto3.client("s3")

RAW_BUCKET = os.getenv("RAW_BUCKET", "gmupatentproject26")
CLEAN_BUCKET = os.getenv("CLEAN_BUCKET", "gmupatentproject26cleandata")

# Input files
RAW_ASSIGNEE_KEY = "Dataset/g_assignee_disambiguated.tsv"
RAW_LOCATION_KEY = "Dataset/g_location_disambiguated.tsv"  # <-- CHECK this name in S3
RAW_APPLICATION_KEY = "Dataset/g_application.tsv"

# Output file (merged)
CLEAN_KEY = "cleaned/g_assignee_location_application_va_clean.csv"

def clean_application_file(df: pd.DataFrame) -> pd.DataFrame:
    keep_columns = ["patent_id", "filing_date"]
    df = df[keep_columns].copy()

    for col in df.select_dtypes(include="object").columns:
        df[col] = df[col].str.replace(r"\t", "", regex=True).str.strip()

    df["filing_date"] = pd.to_datetime(df["filing_date"], errors="coerce")
    df["filing_year"] = df["filing_date"].dt.year
    df = df.drop_duplicates().reset_index(drop=True)
    return df

def clean_assignee_file(df: pd.DataFrame) -> pd.DataFrame:
    """Keep only primary assignee records and selected columns."""
    keep_columns = [
        "patent_id",
        "assignee_id",
        "assignee_type",
        "assignee_sequence",
        "disambig_assignee_individual_name_first",
        "disambig_assignee_individual_name_last",
        "disambig_assignee_organization",
        "location_id",
    ]
    df = df[keep_columns].copy()

    # Clean string columns
    for col in df.select_dtypes(include="object").columns:
        df[col] = df[col].str.replace(r"\t", "", regex=True).str.strip()

    # Keep only primary assignee
    df = df[df["assignee_sequence"] == "0"]

    # Drop duplicates
    df = df.drop_duplicates().reset_index(drop=True)
    return df


def clean_location_file(df: pd.DataFrame) -> pd.DataFrame:
    """Keep only VA locations and selected columns."""
    keep_columns = [
        "location_id",
        "disambig_city",
        "disambig_state",
        "disambig_country",
        "county",
    ]
    df = df[keep_columns].copy()

    # Clean string columns
    for col in df.select_dtypes(include="object").columns:
        df[col] = df[col].str.replace(r"\t", "", regex=True).str.strip()

    # Keep only Virginia records
    df = df[df["disambig_state"] == "VA"]

    # Drop duplicates
    df = df.drop_duplicates().reset_index(drop=True)
    return df


def lambda_handler(event, context):
    # ---------- 1) Read & clean ASSIGNEE ----------
    print(f"Processing assignee file: s3://{RAW_BUCKET}/{RAW_ASSIGNEE_KEY}")

    resp_assignee = s3.get_object(Bucket=RAW_BUCKET, Key=RAW_ASSIGNEE_KEY)
    content_assignee = resp_assignee["Body"].read()

    df_assignee_raw = pd.read_csv(
        io.BytesIO(content_assignee),
        sep="\t",
        dtype="string",
        usecols=[
            "patent_id",
            "assignee_id",
            "assignee_type",
            "assignee_sequence",
            "disambig_assignee_individual_name_first",
            "disambig_assignee_individual_name_last",
            "disambig_assignee_organization",
            "location_id",
        ],
        low_memory=False,
    )

    df_assignee_clean = clean_assignee_file(df_assignee_raw)
    print(f"Assignee rows after cleaning: {len(df_assignee_clean)}")

    # ---------- 2) Read & clean LOCATION ----------
    print(f"Processing location file: s3://{RAW_BUCKET}/{RAW_LOCATION_KEY}")

    resp_location = s3.get_object(Bucket=RAW_BUCKET, Key=RAW_LOCATION_KEY)
    content_location = resp_location["Body"].read()

    df_location_raw = pd.read_csv(
        io.BytesIO(content_location),
        sep="\t",
        dtype="string",
        usecols=[
            "location_id",
            "disambig_city",
            "disambig_state",
            "disambig_country",
            "county",
        ],
        low_memory=False,
    )

    df_location_clean = clean_location_file(df_location_raw)
    print(f"Location rows after cleaning (VA only): {len(df_location_clean)}")

    # ---------- 3) MERGE ASSIGNEE + LOCATION ----------
    df_assignee_loc = pd.merge(
        df_assignee_clean,
        df_location_clean,
        on="location_id",
        how="inner",  # inner = only assignees that have a VA location
    )

    df_assignee_loc = df_assignee_loc.drop_duplicates().reset_index(drop=True)
    print(f"Merged assignee+location (VA only) rows: {len(df_assignee_loc)}")

    # ---------- 4) Read & clean APPLICATION ----------
    print(f"Processing application file: s3://{RAW_BUCKET}/{RAW_APPLICATION_KEY}")

    resp_app = s3.get_object(Bucket=RAW_BUCKET, Key=RAW_APPLICATION_KEY)
    content_app = resp_app["Body"].read()

    df_app_raw = pd.read_csv(
        io.BytesIO(content_app),
        sep="\t",
        dtype="string",
        usecols=["patent_id", "filing_date"],
        low_memory=False,
    )

    df_app_clean = clean_application_file(df_app_raw)
    print(f"Application rows after cleaning: {len(df_app_clean)}")

    # ---------- 5) MERGE WITH APPLICATION ON patent_id ----------
    df_final = pd.merge(
        df_assignee_loc,
        df_app_clean,
        on="patent_id",
        how="left",  # keep all VA assignees; add filing info when available
    )

    df_final = df_final.drop_duplicates().reset_index(drop=True)
    print(f"Final merged rows (assignee + location VA + application): {len(df_final)}")

    # ---------- 6) Write merged CSV to S3 ----------
    csv_bytes = df_final.to_csv(index=False).encode("utf-8")

    s3.put_object(Bucket=CLEAN_BUCKET, Key=CLEAN_KEY, Body=csv_bytes)

    return {
        "statusCode": 200,
        "body": json.dumps(
            {
                "message": "Assignee + Location (VA) + Application merged",
                "rows": int(len(df_final)),
                "output_file": f"s3://{CLEAN_BUCKET}/{CLEAN_KEY}",
            }
        ),
    }
