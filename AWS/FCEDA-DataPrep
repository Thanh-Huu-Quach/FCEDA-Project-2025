import json
import os
import io
import boto3
import pandas as pd

# -------------------------------
# Dataset Prep
# -------------------------------
def clean_application_file(df):
    # Keep only required columns
    keep_columns = ['patent_id', 'filing_date']
    df = df[keep_columns].copy()
    # Remove tabs from string columns
    for col in df.select_dtypes(include='object').columns:
        df[col] = df[col].str.replace(r'\t', '', regex=True).str.strip()
    # Convert filing_date to datetime (same as col_date())
    df['filing_date'] = pd.to_datetime(df['filing_date'], errors='coerce')
    # If you want to extract only the year (like R format "Y"):
    df['filing_year'] = df['filing_date'].dt.year
    # Remove duplicates and reset index
    df = df.drop_duplicates().reset_index(drop=True)
    return df

def clean_assignee_file(df):
    # Keep only required columns
    keep_columns = ['patent_id', 'assignee_id', 'assignee_type', 'assignee_sequence', 'disambig_assignee_individual_name_first', 'disambig_assignee_individual_name_last', 'disambig_assignee_organization', 'location_id']
    df = df[keep_columns].copy()
    # Remove tabs from string columns
    for col in df.select_dtypes(include='object').columns:
        df[col] = df[col].str.replace(r'\t', '', regex=True).str.strip()
    # Keep only primary assignee (assignee_sequence == 0)
    df = df[df['assignee_sequence'] == '0']
    # Remove duplicates and reset index
    df = df.drop_duplicates().reset_index(drop=True)
    return df

def clean_location_file(df):
    # Keep only required columns
    keep_columns = ['location_id', 'disambig_city', 'disambig_state', 'disambig_country', 'county']
    df = df[keep_columns].copy()
    # Remove tabs from string columns
    for col in df.select_dtypes(include='object').columns:
        df[col] = df[col].str.replace(r'\t', '', regex=True).str.strip()
    # Keep only US records (disambig_country == 'VA')
    df = df[df['disambig_state'] == 'VA']
    # Remove duplicates and reset index
    df = df.drop_duplicates().reset_index(drop=True)
    return df

def clean_cpc_current_file(df):
    # Keep only required columns
    keep_columns = ['patent_id', 'cpc_sequence', 'cpc_section', 'cpc_class', 'cpc_subclass', 'cpc_group']
    df = df[keep_columns].copy()
    # Remove tabs from string columns
    for col in df.select_dtypes(include='object').columns:
        df[col] = df[col].str.replace(r'\t', '', regex=True).str.strip()
    # Keep only primary classification (cpc_sequence == 0)
    df = df[df['cpc_sequence'] == '0']
    # Remove duplicates and reset index
    df = df.drop_duplicates().reset_index(drop=True)
    return df

def clean_cpc_title_file(df):
    # Keep only required columns
    keep_columns = ['cpc_class', 'cpc_class_title', 'cpc_subclass', 'cpc_subclass_title', 'cpc_group', 'cpc_group_title']
    df = df[keep_columns].copy()
    # Remove tabs from string columns
    for col in df.select_dtypes(include='object').columns:
        df[col] = df[col].str.replace(r'\t', '', regex=True).str.strip()
    # Remove duplicates and reset index
    df = df.drop_duplicates().reset_index(drop=True)
    return df

# Mapping filename -> cleaning function
CLEANING_STRATEGIES = {
    "g_application.tsv": clean_application_file,
    "g_assignee_disambiguated.tsv": clean_assignee_file,
    "g_location_disambiguated.tsv": clean_location_file,
    "g_cpc_current.tsv": clean_cpc_current_file,
    "g_cpc_title.tsv": clean_cpc_title_file,
}

# Raw bucket where your TSV files live
RAW_BUCKET = os.getenv("RAW_BUCKET", "gmupatentproject26")
RAW_PREFIX = "Dataset/"   # folder in the raw bucket

# Clean bucket where cleaned CSVs will be written
CLEAN_BUCKET = os.getenv("CLEAN_BUCKET", "gmupatentproject26cleandata")

s3 = boto3.client("s3")

# -------------------------------
# CLEANING FUNCTIONS (5 DATASETS)
# -------------------------------

def lambda_handler(event, context):
    """
    Initial clean for 5 patent datasets.
    - Reads TSVs from RAW_BUCKET/Dataset/
    - Applies cleaning functions
    - Writes cleaned CSVs into CLEAN_BUCKET under cleaned/
    """
    print(f"RAW_BUCKET = {RAW_BUCKET}")
    print(f"CLEAN_BUCKET = {CLEAN_BUCKET}")

    # List all objects under RAW_PREFIX
    response = s3.list_objects_v2(Bucket=RAW_BUCKET, Prefix=RAW_PREFIX)
    objects = response.get("Contents", [])

    while response.get("IsTruncated"):
        token = response.get("NextContinuationToken")
        response = s3.list_objects_v2(
            Bucket=RAW_BUCKET,
            Prefix=RAW_PREFIX,
            ContinuationToken=token,
        )
        objects.extend(response.get("Contents", []))

    results = {}

    for obj in objects:
        key = obj["Key"]
        filename = os.path.basename(key)

        # ignore folders / unknown files
        if filename not in CLEANING_STRATEGIES:
            print(f"Skipping {key} (no cleaning strategy).")
            continue

        print(f"Processing {key}...")

        try:
            # Read TSV from S3
            resp = s3.get_object(Bucket=RAW_BUCKET, Key=key)
            content = resp["Body"].read()
            df_raw = pd.read_csv(
                io.BytesIO(content),
                sep="\t",
                dtype="string",
                low_memory=False,
            )

            # Clean using appropriate function
            cleaning_fn = CLEANING_STRATEGIES[filename]
            df_clean = cleaning_fn(df_raw)

            # Build cleaned key name in clean bucket
            cleaned_key = f"cleaned/{filename.replace('.tsv', '_clean.csv')}"
            cleaned_bytes = df_clean.to_csv(index=False).encode("utf-8")

            # Upload cleaned CSV
            s3.put_object(
                Bucket=CLEAN_BUCKET,
                Key=cleaned_key,
                Body=cleaned_bytes,
            )

            print(f"Uploaded cleaned file to s3://{CLEAN_BUCKET}/{cleaned_key}")
            results[filename] = {
                "rows": int(len(df_clean)),
                "cleaned_key": cleaned_key,
            }

        except Exception as e:
            print(f"Error processing {key}: {e}")
            results[filename] = {"error": str(e)}

    return {
        "statusCode": 200,
        "body": json.dumps(
            {
                "message": "Initial clean complete for up to 5 patent datasets.",
                "raw_bucket": RAW_BUCKET,
                "clean_bucket": CLEAN_BUCKET,
                "results": results,
            }
        ),
    }
